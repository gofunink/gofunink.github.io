<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SparkRDD转DataFrame的两种方式]]></title>
    <url>%2F2019%2F03%2F20%2FSparkRDD%E8%BD%ACDataFrame%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[SparkRDD转DataFrame 映射的方式1234567891011121314151617181920212223242526272829303132333435363738394041package com.gofun.sparkSqlimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.SQLContext/** * Create by IntelliJ IDEA. * Author gofun * 2017/10/10 20:18 */object RDD2DataFrameReflection &#123; Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.apache.eclipse.jetty.server&quot;).setLevel(Level.OFF) def main(args: Array[String]): Unit = &#123; rdd2DataFrame() &#125; //rdd与dataframe转换 //在scala中使用反射方式，进行RDD到DataFrame转换，需要手动导入 def rdd2DataFrame(): Unit = &#123; val conf = new SparkConf().setAppName(&quot;rdd2DataFrame&quot;).setMaster(&quot;local[2]&quot;) conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;192.168.157.200:2181&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) // System.setProperty(&quot;hadoop.home.dir&quot;, &quot;E:\\hadoop-2.5.0-cdh5.3.6&quot;) val lines = sc.textFile(&quot;hdfs://192.xxx.xxx.200:8020/test/student.txt&quot;) val students = lines.map(_.split(&quot;\t&quot;)) .map &#123; line =&gt; Student(line(0).trim().toInt, line(1).trim()) &#125; val studentDF = sqlContext.createDataFrame(students) studentDF.registerTempTable(&quot;studentTable&quot;) val df = sqlContext.sql(&quot;select * from studentTable&quot;) df.rdd.collect().foreach(println) sc.stop() &#125;&#125;case class Student(id: Int, name: String) SparkRDD转DataFrame 构造元数据的方式1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.gofun.sparkSqlimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.&#123;Row, RowFactory, SQLContext&#125;import org.apache.spark.sql.types._/** * Create by IntelliJ IDEA. * Author gofun * 2017/10/10 20:19 */object RDD2DataFrameProgrammatically extends App &#123; Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.apache.eclipse.jetty.server&quot;).setLevel(Level.OFF) override def main(args: Array[String]): Unit = &#123; rdd2DataFrame() &#125; //构造元数据的方式加载数据将rdd转换为dataFrame def rdd2DataFrame(): Unit = &#123; val conf = new SparkConf().setAppName(&quot;RDD2DataFrameProgrammatically&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val lines = sc.textFile(&quot;hdfs://192.xxx.xx.200:8020/test/student.txt&quot;) val studentRdd = lines.map(_.split(&quot; &quot;)).map(line =&gt; RowFactory.create(Integer.valueOf(line(0)), String.valueOf(line(1)), Integer.valueOf(line(2)))) val fields = new scala.collection.mutable.ArrayBuffer[StructField]() fields += DataTypes.createStructField(&quot;id&quot;, DataTypes.IntegerType, true) fields += DataTypes.createStructField(&quot;name&quot;, DataTypes.StringType, true) fields += DataTypes.createStructField(&quot;age&quot;, DataTypes.IntegerType, true) val structType = DataTypes.createStructType(fields.toArray) val studentDF = sqlContext.createDataFrame(studentRdd, structType) studentDF.registerTempTable(&quot;student&quot;) val df = sqlContext.sql(&quot;select name from student&quot;) df.rdd.collect().foreach(println) sc.stop() &#125; def rdd2DataFrame2(): Unit = &#123; val conf = new SparkConf().setAppName(&quot;RDD2DataFrameProgrammatically&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) val lines = sc.textFile(&quot;hdfs://192.168.64.200:8020/test/student.txt&quot;, 1) val studentRDD = lines.map(line =&gt; Row(line.split(&quot; &quot;)(0).toInt, line.split(&quot; &quot;)(1), line.split(&quot; &quot;)(2).toInt)) val structType = StructType(Array(StructField(&quot;id&quot;, IntegerType, true), StructField(&quot;name&quot;, StringType, true), StructField(&quot;age&quot;, IntegerType, true))) val studentDF = sqlContext.createDataFrame(studentRDD, structType) studentDF.registerTempTable(&quot;student&quot;) val df = sqlContext.sql(&quot;select name,age from student&quot;) df.rdd.foreach(println) &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>SparkRDD</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive时间函数]]></title>
    <url>%2F2019%2F03%2F20%2FHive%E6%97%B6%E9%97%B4%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[当前时间123456语法: select from_unixtime(unix_timestamp(),&apos;yyyy-MM-dd HH:mm:ss’)返回值: stringhive&gt; select from_unixtime(1323308943,’yyyyMMdd’) from dual;2019-03-19 17:37:03 UNIX时间戳转日期函数: from_unixtime1234567语法: from_unixtime(bigint unixtime[, string format]) 返回值: string说明: 转化UNIX时间戳（从1970-01-01 00:00:00 UTC到指定时间的秒数）到当前时区的时间格式举例：hive&gt; select from_unixtime(1323308943,’yyyyMMdd’) from dual;20111208 获取当前UNIX时间戳函数: unix_timestamp1234567语法: unix_timestamp() 返回值: bigint说明: 获得当前时区的UNIX时间戳举例：hive&gt; select unix_timestamp() from dual;1323309615 日期转UNIX时间戳函数: unix_timestamp1234567语法: unix_timestamp(string date) 返回值: bigint说明: 转换格式为“yyyy-MM-dd HH:mm:ss“的日期到UNIX时间戳。如果转化失败，则返回0。举例：hive&gt; select unix_timestamp(’2011-12-07 13:01:03′) from dual;1323234063 指定格式日期转UNIX时间戳函数: unix_timestamp1234567语法: unix_timestamp(string date, string pattern) 返回值: bigint说明: 转换pattern格式的日期到UNIX时间戳。如果转化失败，则返回0。举例：hive&gt; select unix_timestamp(’20111207 13:01:03′,’yyyyMMdd HH:mm:ss’) from dual;1323234063 日期时间转日期函数: to_date1234567语法: to_date(string timestamp) 返回值: string说明: 返回日期时间字段中的日期部分。举例：hive&gt; select to_date(’2011-12-08 10:03:01′) from dual;2011-12-08 日期转年函数: year123456789语法: year(string date) 返回值: int说明: 返回日期中的年。举例：hive&gt; select year(’2011-12-08 10:03:01′) from dual;2011hive&gt; select year(’2012-12-08′) from dual;2012 日期转月函数: month123456789语法: month (string date) 返回值: int说明: 返回日期中的月份。举例：hive&gt; select month(’2011-12-08 10:03:01′) from dual;12hive&gt; select month(’2011-08-08′) from dual;8 日期转天函数: day123456789语法: day (string date) 返回值: int说明: 返回日期中的天。举例：hive&gt; select day(’2011-12-08 10:03:01′) from dual;8hive&gt; select day(’2011-12-24′) from dual;24 日期转小时函数: hour1234567语法: hour (string date) 返回值: int说明: 返回日期中的小时。举例：hive&gt; select hour(’2011-12-08 10:03:01′) from dual;10 日期转分钟函数: minute1234567语法: minute (string date) 返回值: int说明: 返回日期中的分钟。举例：hive&gt; select minute(’2011-12-08 10:03:01′) from dual;3 日期转秒函数: second1234567语法: second (string date) 返回值: int说明: 返回日期中的秒。举例：hive&gt; select second(’2011-12-08 10:03:01′) from dual;1 日期转周函数: weekofyear1234567语法: weekofyear (string date) 返回值: int说明: 返回日期在当前的周数。举例：hive&gt; select weekofyear(’2011-12-08 10:03:01′) from dual;49 日期比较函数: datediff1234567语法: datediff(string enddate, string startdate) 返回值: int说明: 返回结束日期减去开始日期的天数。举例：hive&gt; select datediff(’2012-12-08′,’2012-05-09′) from dual;213 日期增加函数: date_add1234567语法: date_add(string startdate, int days) 返回值: string说明: 返回开始日期startdate增加days天后的日期。举例：hive&gt; select date_add(’2012-12-08′,10) from dual;2012-12-18 日期减少函数: date_sub1234567语法: date_sub (string startdate, int days) 返回值: string说明: 返回开始日期startdate减少days天后的日期。举例：hive&gt; select date_sub(’2012-12-08′,10) from dual;2012-11-28 yyyyMMdd转成yyyy-MM-dd12345678910111213方法一 语法: from_unixtime+ unix_timestamp返回值：string举例：hive&gt; select from_unixtime(unix_timestamp(&apos;20171205&apos;,&apos;yyyymmdd&apos;),&apos;yyyy-mm-dd&apos;) from dual;2017-12-05方法二 语法: substr + concat返回值：string举例：hive&gt; select concat(substr(&apos;20171205&apos;,1,4),&apos;-&apos;,substr(&apos;20171205&apos;,5,2),&apos;-&apos;,substr(&apos;20171205&apos;,7,2)) from dual;2017-12-05 yyyy-MM-dd转成yyyyMMdd12345678910111213方法一 语法: from_unixtime+ unix_timestamp返回值：string举例：hive&gt; select from_unixtime(unix_timestamp(&apos;2017-12-05&apos;,&apos;yyyy-mm-dd&apos;),&apos;yyyymmdd&apos;) from dual;20171205方法二 语法: substr + concat返回值：string举例：hive&gt; select concat(substr(&apos;2017-12-05&apos;,1,4),substr(&apos;2017-12-05&apos;,6,2),substr(&apos;2017-12-05&apos;,9,2)) from dual;20171205]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>时间函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql数据导出到Hbase]]></title>
    <url>%2F2019%2F03%2F08%2FMysql%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%E5%88%B0Hbase%2F</url>
    <content type="text"><![CDATA[Mysql表center_pub_merchant数据导出为center_pub_merchant.tsv1mysql -h 172.xx.x.175 -P 3307 -uUSER -pPASSWORD -e &quot;SELECT concat(&apos;merchant:&apos;,id), parentId, id, cascade_label AS cascadeLabel, cascade_level AS cascadeLevel, account_id AS accountId, merchant_name AS merchantName,merchant_type AS merchantType,merchant_project AS merchantProject, full_name AS fullName, telephone, email, thumb, industry, user_id AS userId, province,city, county FROM center_pub_merchant&quot; awifi_dc &gt; /opt/gofunTest/center_pub_merchant.tsv1hdfs dfs -put center_pub_merchant.tsv /tmp/center_pub_merchant.tsv1hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,base_data:parentId,base_data:id,base_data:cascadeLabel,base_data:cascadeLevel,base_data:accountId,base_data:merchantName,base_data:merchantType,base_data:merchantProject,base_data:fullName,base_data:telephone,base_data:email,base_data:thumb,base_data:industry,base_data:userId,base_data:province,base_data:city,base_data:county tj_base_merchant /tmp/center_pub_merchant.tsvMysql表center_pub_device数据导出为center_pub_device.tsv1mysql -h 172.XX.X.175 -P 3307 -uUSER -pPASSWORD -e &quot;SELECT concat(&apos;device:&apos;,d.device_id),d.province,d.city,d.county,e.entity_type as entityType,d.device_id as deviceId,d.project_id as projectId,d.belongto FROM center_pub_device d INNER JOIN center_pub_entity e ON d.OUT_ID = e.ID WHERE d.OUT_TYPE_ID = &apos;00&apos;&quot; awifi_dc &gt; /opt/gofunTest/center_pub_device.tsv1hdfs dfs -put center_pub_device.tsv /tmp/center_pub_device.tsv1hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,base_data:province,base_data:city,base_data:county,base_data:entityType,base_data:deviceId,base_data:projectId,base_data:belongto tj_base_device /tmp/center_pub_device.tsv注意： 若Permission denied: user=hbase, access=WRITE, inode=”/user”:hdfs:supergroup:drwxr-xr-x 修改 hadoop fs -chmod 777 /user 若报min.user.id。。。的错 找到这个文件/etc/hadoop/conf.cloudera.yarn/container-executor.cfg chmod 755 container-executor.cfg 将 min.user.id=900改小 chmod 400 container-executor.cfg* 若报banned.users。。。的错 chmod 755 container-executor.cfg 则将banned.users=hdfs,yarn,mapred,bin中hdfs删掉 chmod 400 container-executor.cfg]]></content>
      <categories>
        <category>Mysql</category>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive的优化总结]]></title>
    <url>%2F2019%2F03%2F07%2FHive%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[# HIVE的优化总结Hive是将符合SQL语法的字符串解析生成可以在Hadoop上执行的MapReduce的工具。使用Hive尽量按照分布式计算的一些特点来设计sql，和传统关系型数据库有区别，所以需要去掉原有关系型数据库下开发的一些固有思维。基本原则：1、尽量尽早地过滤数据，减少每个阶段的数据量,对于分区表要加分区，同时只选择需要使用到的字段12345678910111213select ... from Ajoin Bon A.key = B.keywhere A.userid&gt;10 and B.userid&lt;10 and A.dt=&apos;20120417&apos; and B.dt=&apos;20120417&apos;;应该改写为：123456789101112131415161718select .... from (select .... from A where dt=&apos;201200417&apos; and userid&gt;10 ) ajoin ( select .... from B where dt=&apos;201200417&apos; and userid &lt; 10 ) bon a.key = b.key;2、对历史库的计算经验 (这项是说根据不同的使用目的优化使用方法) 历史库计算和使用，分区3、尽量原子化操作，尽量避免一个SQL包含复杂逻辑 可以使用中间表来完成复杂的逻辑4、jion操作 小表要注意放在join的左边（目前TCL里面很多都小表放在join的右边）。 否则会引起磁盘和内存的大量消耗5、如果union all的部分个数大于2，或者每个union部分数据量大，应该拆成多个insert into 语句，实际测试过程中，执行时间能提升50%1234567891011121314151617insert overwite table tablename partition (dt= ....)select ..... from ( select ... from A union all select ... from B union all select ... from C ) Rwhere ...;可以改写为：1234567891011121314151617insert into table tablename partition (dt= ....)select .... from AWHERE ...;insert into table tablename partition (dt= ....)select .... from BWHERE ...;insert into table tablename partition (dt= ....)select .... from CWHERE ...;6、写SQL要先了解数据本身的特点，如果有join ,group操作的话，要注意是否会有数据倾斜 - 如果出现数据倾斜，应当做如下处理：1234567891011set hive.exec.reducers.max=200;set mapred.reduce.tasks= 200; ---增大Reduce个数set hive.groupby.mapaggr.checkinterval=100000; --这个是group的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置set hive.groupby.skewindata=true; --如果是group by过程出现倾斜 应该设置为trueset hive.skewjoin.key=100000; --这个是join的键对应的记录条数超过这个值则会进行分拆,值根据具体数据量设置set hive.optimize.skewjoin=true; --如果是join 过程出现倾斜 应该设置为true (1) 启动一次job尽可能的多做事情，一个job能完成的事情,不要两个job来做 通常来说前面的任务启动可以稍带一起做的事情就一起做了,以便后续的多个任务重用,与此紧密相连的是模型设计,好的模型特别重要. (2) 合理设置reduce个数 reduce个数过少没有真正发挥hadoop并行计算的威力，但reduce个数过多，会造成大量小文件问题 ，数据量、资源情况只有自己最清楚，找到个折衷点, (3) 使用hive.exec.parallel参数控制在同一个sql中的不同的job是否可以同时运行，提高作业的并发- 让服务器尽量少做事情，走最优的路径，以资源消耗最少为目标(1) 注意join的使用 若其中有一个表很小使用map join，否则使用普通的reduce join，注意hive会将join前面的表数据装载内存,所以较小的一个表在较大的表之前,减少内存资源的消耗(2)注意小文件的问题 在hive里有两种比较常见的处理办法 第一是使用Combinefileinputformat，将多个小文件打包作为一个整体的inputsplit，减少map任务数 set mapred.max.split.size=256000000; set mapred.min.split.size.per.node=256000000 set Mapred.min.split.size.per.rack=256000000 set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat 第二是设置hive参数，将额外启动一个MR Job打包小文件 hive.merge.mapredfiles = false 是否合并 Reduce 输出文件，默认为 False hive.merge.size.per.task = 25610001000 合并文件的大小 (3)注意数据倾斜 在hive里比较常用的处理办法 第一通过hive.groupby.skewindata=true控制生成两个MR Job,第一个MR Job Map的输出结果随机分配到reduce做次预汇总,减少某些key值条数过多某些key条数过小造成的数据倾斜问题 第二通过hive.map.aggr = true(默认为true)在Map端做combiner,假如map各条数据基本上不一样, 聚合没什么意义，做combiner反而画蛇添足,hive里也考虑的比较周到通过参数hive.groupby.mapaggr.checkinterval = 100000 (默认)hive.map.aggr.hash.min.reduction=0.5(默认),预先取100000条数据聚合,如果聚合后的条数/100000&gt;0.5，则不再聚合(4)善用multi insert,union all multi insert适合基于同一个源表按照不同逻辑不同粒度处理插入不同表的场景，做到只需要扫描源表一次，job个数不变，减少源表扫描次数 union all用好，可减少表的扫描次数，减少job的个数,通常预先按不同逻辑不同条件生成的查询union all后，再统一group by计算,不同表的union all相当于multiple inputs,同一个表的union all,相当map一次输出多条(5) 参数设置的调优 集群参数种类繁多,举个例子比如 可针对特定job设置特定参数,比如jvm重用,reduce copy线程数量设置(适合map较快，输出量较大) 如果任务数多且小，比如在一分钟之内完成，减少task数量以减少任务初始化的消耗。可以通过配置JVM重用选项减少task的消耗一、控制Hive中Map和reduce的数量 Hive中的sql查询会生成执行计划，执行计划以MapReduce的方式执行，那么结合数据和集群的大小，map和reduce的数量就会影响到sql执行的效率。 除了要控制Hive生成的Job的数量，也要控制map和reduce的数量。1、map的数量，通常情况下和split的大小有关系1234567 hive中默认的hive.input.format是org.apache.hadoop.hive.ql.io.CombineHiveInputFormat，对于combineHiveInputFormat,它的输入的map数量由三个配置决定，mapred.min.split.size.per.node， 一个节点上split的至少的大小mapred.min.split.size.per.rack 一个交换机下split至少的大小mapred.max.split.size 一个split最大的大小它的主要思路是把输入目录下的大文件分成多个map的输入, 并合并小文件, 做为一个map的输入. 具体的原理是下述三步: a、根据输入目录下的每个文件,如果其长度超过mapred.max.split.size,以block为单位分成多个split(一个split是一个map的输入),每个split的长度都大于mapred.max.split.size, 因为以block为单位, 因此也会大于blockSize, 此文件剩下的长度如果大于mapred.min.split.size.per.node, 则生成一个split, 否则先暂时保留. b、现在剩下的都是一些长度效短的碎片,把每个rack下碎片合并, 只要长度超过mapred.max.split.size就合并成一个split, 最后如果剩下的碎片比mapred.min.split.size.per.rack大, 就合并成一个split, 否则暂时保留. c、把不同rack下的碎片合并, 只要长度超过mapred.max.split.size就合并成一个split, 剩下的碎片无论长度, 合并成一个split.举例: mapred.max.split.size=1000 mapred.min.split.size.per.node=300 mapred.min.split.size.per.rack=100 输入目录下五个文件,rack1下三个文件,长度为2050,1499,10, rack2下两个文件,长度为1010,80. 另外blockSize为500. 经过第一步, 生成五个split: 1000,1000,1000,499,1000. 剩下的碎片为rack1下:50,10; rack2下10:80 由于两个rack下的碎片和都不超过100, 所以经过第二步, split和碎片都没有变化. 第三步,合并四个碎片成一个split, 长度为150. 如果要减少map数量, 可以调大mapred.max.split.size, 否则调小即可. 其特点是: 一个块至多作为一个map的输入，一个文件可能有多个块，一个文件可能因为块多分给做为不同map的输入， 一个map可能处理多个块，可能处理多个文件。2、reduce数量可以在hive运行sql的时，打印出来，如下：12345678910111213Number of reduce tasks not specified. Estimated from input data size: 1In order to change the average load for a reducer (in bytes): set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;In order to limit the maximum number of reducers: set hive.exec.reducers.max=&lt;number&gt;In order to set a constant number of reducers: set mapred.reduce.tasks=&lt;number&gt;reduce数量由以下三个参数决定： mapred.reduce.tasks (强制指定reduce的任务数量) hive.exec.reducers.bytes.per.reducer （每个reduce任务处理的数据量，默认为1000^3=1G) hive.exec.reducers.max（每个任务最大的reduce数，默认为999） 计算reducer数的公式很简单N=min( hive.exec.reducers.max ，总输入数据量/ hive.exec.reducers.bytes.per.reducer ) 只有一个reduce的场景： a、没有group by 的汇总 b、order by c、笛卡尔积二、join和Group的优化 对于普通的join操作，会在map端根据key的hash值，shuffle到某一个reduce上去，在reduce端做join连接操作，内存中缓存join左边的表，遍历右边的表，一次做join操作。所以在做join操作时候，将数据量多的表放在join的右边。 当数据量比较大，并且key分布不均匀，大量的key都shuffle到一个reduce上了，就出现了数据的倾斜。 对于Group操作，首先在map端聚合，最后在reduce端坐聚合，hive默认是这样的，以下是相关的参数 · hive.map.aggr = true是否在 Map 端进行聚合，默认为 True · hive.groupby.mapaggr.checkinterval = 100000在 Map 端进行聚合操作的条目数目对于join和Group操作都可能会出现数据倾斜。 以下有几种解决这个问题的常见思路 1、参数hive.groupby.skewindata = true,解决数据倾斜的万能钥匙，查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。 2、where的条件写在join里面，使得减少join的数量（经过map端过滤，只输出复合条件的） 3、mapjoin方式，无reduce操作，在map端做join操作（map端cache小表的全部数据），这种方式下无法执行Full/RIGHT OUTER join操作 4、对于count(distinct)操作，在map端以group by的字段和count的字段联合作为key，如果有大量相同的key，那么会存在数据倾斜的问题 5、数据的倾斜还包括，大量的join连接key为空的情况，空的key都hash到一个reduce上去了，解决这个问题，最好把空的key和非空的key做区分，空的key不做join操作。 当然有的hive操作，不存在数据倾斜的问题，比如数据聚合类的操作，像sum、count，因为已经在map端做了聚合操作了，到reduce端的数据相对少一些，所以不存在这个问题。三、小文件的合并 大量的小文件导致文件数目过多，给HDFS带来压力，对hive处理的效率影响比较大，可以合并map和reduce产生的文件 hive.merge.mapfiles = true 是否和并 Map 输出文件，默认为 True hive.merge.mapredfiles = false 是否合并 Reduce 输出文件，默认为 False hive.merge.size.per.task = 25610001000 合并文件的大小四、in/exists（not） 通过left semi join 实现 in操作，一个限制就是join右边的表只能出现在join条件中五、分区裁剪 通过在条件中指定分区，来限制数据扫描的范围，可以极大提高查询的效率六、排序 order by 排序，只存在一个reduce，这样效率比较低。 可以用sort by操作,通常结合distribute by使用做reduce分区键如想了解更多，请参考原文地址：HIVE的优化总结]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用MackDown语法写Hexo博客]]></title>
    <url>%2F2017%2F11%2F10%2F%E7%94%A8MackDown%E8%AF%AD%E6%B3%95%E5%86%99hexo%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[「简书」作为一款「写作软件」在诞生之初就支持了 Markdown，Markdown 是一种「电子邮件」风格的「标记语言」，我们强烈推荐所有写作者学习和掌握该语言。为什么？可以参考: 『为什么作家应该用 Markdown 保存自己的文稿』。 『Markdown写作浅谈』 在此，我们总结 Markdown 的优点如下： 纯文本，所以兼容性极强，可以用所有文本编辑器打开。 让你专注于文字而不是排版。 格式转换方便，Markdown 的文本你可以轻松转换为 html、电子书等。 Markdown 的标记语法有极好的可读性。 当然，我们既然如此推崇 Markdown ，也必定会教会你使用 Markdown ，这也是本文的目的所在。不过，虽然 Markdown 的语法已经足够简单，但是现有的 Markdown 语法说明更多的是写给 web 从业者看的，对于很多写作者来说，学习起来效率很低，现在，我们特地为写作者量身定做本指南，从写作者的实际需求出发，介绍写作者真正实用的常用格式，深入浅出、图文并茂地让您迅速掌握 Markdown 语法。 为了使您更好地学习，我们建议您登录「简书」，将您的编辑器切换至 Markdown 编辑器，新建一篇空白笔记，然后点击右上角的预览模式： 此时，您的界面应当如下图所示，左侧为编辑区域，右侧为预览区域，您在左侧输入 Markdown 语法的文本，右侧会立即帮您呈现最终结果，好了，让我们开始学习吧~ 标题这是最为常用的格式，在平时常用的的文本编辑器中大多是这样实现的：输入文本、选中文本、设置标题格式。 而在 Markdown 中，你只需要在文本前面加上 # 即可，同理、你还可以增加二级标题、三级标题、四级标题、五级标题和六级标题，总共六级，只需要增加 # 即可，标题字号相应降低。例如： 123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 注：# 和「一级标题」之间建议保留一个字符的空格，这是最标准的 Markdown 写法。 你可以你的编辑器中尝试输入这六级标题，可以参考下方的截图： 列表列表格式也很常用，在 Markdown 中，你只需要在文字前面加上 - 就可以了，例如： 123- 文本1- 文本2- 文本3 如果你希望有序列表，也可以在文字前面加上 1. 2. 3. 就可以了，例如： 1231\. 文本12\. 文本23\. 文本3 注：-、1.和文本之间要保留一个字符的空格。 列表案例截图如下： 链接和图片在 Markdown 中，插入链接不需要其他按钮，你只需要使用 [显示文本](链接地址) 这样的语法即可，例如： 1[简书](http://www.jianshu.com) 在 Markdown 中，插入图片不需要其他按钮，你只需要使用 [图片上传失败...(image-5fdc5-1510890177031)] 这样的语法即可，例如： 1![](http://upload-images.jianshu.io/upload_images/259-0ad0d0bfc1c608b6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 注：插入图片的语法和链接的语法很像，只是前面多了一个 ！。 插入链接和图片的案例截图： 引用在我们写作的时候经常需要引用他人的文字，这个时候引用这个格式就很有必要了，在 Markdown 中，你只需要在你希望引用的文字前面加上 &gt; 就好了，例如： 1&gt; 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 注：&gt; 和文本之间要保留一个字符的空格。 最终显示的就是： 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 引用的案例截图： 粗体和斜体Markdown 的粗体和斜体也非常简单，用两个 * 包含一段文本就是粗体的语法，用一个 * 包含一段文本就是斜体的语法。例如： 1*一盏灯*， 一片昏黄；**一简书**， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 最终显示的就是下文，其中「一盏灯」是斜体，「一简书」是粗体： 一盏灯， 一片昏黄；一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 粗体和斜体的案例截图： 代码引用需要引用代码时，如果引用的语句只有一段，不分行，可以用 ` 将语句包起来。如果引用的语句为多行，可以将1234567*代码引用的案例截图：*![](http://upload-images.jianshu.io/upload_images/259-dcf737a97e71cd73.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)##表格相关代码： Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 12345678910显示效果：|Tables | Are | Cool || ------------- |:-------------:| -----:|| col 3 is | right-aligned | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 |相关代码： dog bird cat foo foo foo bar bar bar baz baz baz 12345678910111213显示效果：dog | bird | cat----|------|----foo | foo | foobar | bar | barbaz | baz | baz## 显示链接中带括号的图片![](http://latex.codecogs.com/gif.latex?%5Cprod%20%5C(n_%7Bi%7D%5C)+1)代码如下: ![][1][1]: http://latex.codecogs.com/gif.latex?\prod%20\(n_{i}\)+1` 结语以上几种格式是比较常用的格式，所以我们针对这些语法做了比较详细的说明。除这些之外，Markdown 还有其他语法，如想了解和学习更多，可以参考这篇『Markdown 语法说明』。 原文作者：简书 链接：http://www.jianshu.com/p/q81RER]]></content>
      <categories>
        <category>MackDown</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
